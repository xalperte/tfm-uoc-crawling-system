{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Spark + DSE\n",
    "\n",
    "In BGDS, we advocate for the convination of the following tools as the best solution to address Big data's problems.\n",
    "\n",
    "- Apache Spark\n",
    "- Apache Cassandra (DSE)\n",
    "- Apache Solr (DSE)\n",
    "\n",
    "This notebook is an example of how to use these tools together to analyze the data crawled by the `Bovespa` example crawler included in the DaVinci distribution ().\n",
    "\n",
    "In order to be able to execute this notebook you will need first to start the `Bovespa` crawler and crawl all the data.\n",
    "\n",
    "```bash\n",
    "python manage.py crawl bovespa \\\n",
    "    --workers-num 10 \\\n",
    "    --chromium-bin-file '/Applications/Chromium.app/Contents/MacOS/Chromium' \\\n",
    "    --io-gs-project centering-badge-212119 \\\n",
    "    --cache-dir \"gs://davinci_example_bovespa\" \\\n",
    "    --local-dir \"fs:///data/bovespa/local\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations related to Cassandra connector & Cluster\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages datastax:spark-cassandra-connector:2.4.0-s_2.11 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark Context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf() \\\n",
    " .setAppName(\"Factor Analysis Job\") \\\n",
    " .set(\"spark.cassandra.connection.host\", \"dse_seed_backend\") \\\n",
    " .set(\"spark.sql.dse.search.enableOptimization\", \"on\") \n",
    "# .set(\"spark.cassandra.auth.username\", \"cassandra\") \\\n",
    "# .set(\"spark.cassandra.auth.password\", \"sQQE87Nt\")  \n",
    "\n",
    "# spark_master = \"spark://127.0.0.1:7077\"\n",
    "spark_master = \"local\"\n",
    "sc = SparkContext(spark_master, \"Factor Analysis Job\", conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating PySpark SQL Context\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for return a data frame attached to the informed keyspace.table\n",
    "def load_and_get_table_df(keys_space_name, table_name):\n",
    "    table_df = sqlContext.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(table=table_name, keyspace=keys_space_name) \\\n",
    "        .option(\"spark.sql.dse.search.enableOptimization\", \"on\") \\\n",
    "        .load()\n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Cannot build a cluster without contact points'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o38.load.\n: java.lang.IllegalArgumentException: Cannot build a cluster without contact points\n\tat com.datastax.driver.core.Cluster.checkNotEmpty(Cluster.java:119)\n\tat com.datastax.driver.core.Cluster.<init>(Cluster.java:112)\n\tat com.datastax.driver.core.Cluster.buildFrom(Cluster.java:178)\n\tat com.datastax.driver.core.Cluster$Builder.build(Cluster.java:1299)\n\tat com.datastax.spark.connector.cql.DefaultConnectionFactory$.createCluster(CassandraConnectionFactory.scala:131)\n\tat com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:159)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)\n\tat com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)\n\tat com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)\n\tat com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)\n\tat com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)\n\tat com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:79)\n\tat com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:111)\n\tat com.datastax.spark.connector.rdd.partitioner.dht.TokenFactory$.forSystemLocalPartitioner(TokenFactory.scala:98)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation$.apply(CassandraSourceRelation.scala:272)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:56)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e7dc96d3d1d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Loading Bovespa companies table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcompanies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_get_table_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"davinci\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bovespa_company\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcompanies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"situation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meqNullSafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CANCELED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6a61c34f651e>\u001b[0m in \u001b[0;36mload_and_get_table_df\u001b[0;34m(keys_space_name, table_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.apache.spark.sql.cassandra\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys_space_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.dse.search.enableOptimization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"on\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtable_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Cannot build a cluster without contact points'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# Loading Bovespa companies table\n",
    "companies = load_and_get_table_df(\"davinci\", \"bovespa_company\")\n",
    "companies.filter(~col(\"situation\").eqNullSafe(\"CANCELED\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the execution plan of the query\n",
    "companies.filter(~col(\"situation\").eqNullSafe(\"CANCELED\")).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing to the fundamental data of the companies\n",
    "# Attaching a data frame to the accounting notes available in the database\n",
    "fundamentals = load_and_get_table_df(\"davinci\", \"bovespa_account\")\n",
    "fundamentals.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "#factors_df = fundamentals.filter(\n",
    "#    col(\"number\").isin([\"6.02\",\"1\"]) & col(\"ccvm\").isin([\"13773\", \"22306\", \"1023\", \"10472\"]))\n",
    "\n",
    "# Get the Capital Expenditures from the Fundamentals\n",
    "# Account 1    = Total Assets of the company\n",
    "# Account 6.02 = Capital Expenditure reported by the company during the period\n",
    "factors_df = fundamentals.filter(\n",
    "    col(\"number\").isin([\"6.02\", \"1\"]))\n",
    "factors_df = factors_df.withColumn(\n",
    "    \"factor_name\", when(factors_df.number == \"1\", \"total_assets\").otherwise(\"cap_ex_reported\"))\n",
    "\n",
    "factors_df = factors_df\\\n",
    "    .select(col(\"ccvm\").alias(\"asset\"), \n",
    "            col(\"period\").alias(\"astodate\"),\n",
    "            col(\"factor_name\"), \n",
    "            col(\"amount\").alias(\"amount\"))\n",
    "\n",
    "factors_df = factors_df.groupby(col(\"asset\"), col(\"astodate\"))\\\n",
    "    .pivot(\"factor_name\").sum(\"amount\").orderBy(\"asset\", \"astodate\")\n",
    "\n",
    "factors_df = factors_df.withColumn(\n",
    "    \"cap_ex_reported_scaled\", \n",
    "    when(factors_df.cap_ex_reported > 0, 0).otherwise(abs(factors_df.cap_ex_reported)) / factors_df.total_assets)\n",
    "\n",
    "factors_df = factors_df.withColumn(\"capex_vol_6q\", stddev(col(\"cap_ex_reported_scaled\"))\n",
    "             .over(Window.partitionBy(\"asset\").rowsBetween(-5, 0)) )\n",
    "\n",
    "factors_df = factors_df.withColumn(\"capex_vol_6q_ranked\", rank()\\\n",
    "             .over(Window.partitionBy(\"astodate\").orderBy(asc(\"capex_vol_6q\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df.select(col(\"astodate\"), col(\"asset\"), col(\"capex_vol_6q\"), col(\"capex_vol_6q_ranked\")).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_w(col, w):\n",
    "    avg_ = avg(col).over(w)\n",
    "    avg_sq = avg(col * col).over(w)\n",
    "    sd_ = sqrt(avg_sq - avg_ * avg_)\n",
    "    return (col - avg_) / sd_\n",
    "\n",
    "w = Window().partitionBy(\"astodate\")\n",
    "factors_df_standard = factors_df.withColumn(\"capex_vol_6q_ranked_zscored\", \n",
    "                                            z_score_w(factors_df.capex_vol_6q_ranked, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df_standard.select(\n",
    "    col(\"astodate\"), \n",
    "    col(\"asset\"), \n",
    "    col(\"capex_vol_6q_ranked_zscored\")).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_data = factors_df_standard.select(\n",
    "    col(\"astodate\"), \n",
    "    col(\"asset\"), \n",
    "    col(\"capex_vol_6q_ranked_zscored\").alias(\"capex_vol_6q\")).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist the results\n",
    "\n",
    "Now it's time to save the factor into a table. To do this, the system should had create a \n",
    "keyspace for us in the database. The keyspace should have the same TOKEN than the one used\n",
    "to connect through the API.\n",
    "\n",
    "Each user will have it's own space in the system to run the analyses.\n",
    "\n",
    "```sql\n",
    "CREATE KEYSPACE IF NOT EXISTS <USER_ID>_analysis WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1};\n",
    "```\n",
    "\n",
    "with full access to it:\n",
    "\n",
    "```sql\n",
    "-- Create the user using the token and\n",
    "CREATE ROLE user_<USER_ID> WITH PASSWORD = '<USER_TOKEN>' AND LOGIN = true AND SUPERUSER = false;\n",
    "\n",
    "-- Grant all permission for the user keyspace\n",
    "GRANT ALL PERMISSIONS IN KEYSPACE <USER_ID>_analysis to user_<USER_ID>;\n",
    "```\n",
    "\n",
    "PRO features:\n",
    "- Allow replication of data (ReplicationFactor = 3)\n",
    "    ```sql\n",
    "    CREATE KEYSPACE IF NOT EXISTS <TOKEN>_analysis WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3};\n",
    "    ```\n",
    "- Network replication (multiple DataCenters)\n",
    "    ```sql\n",
    "    CREATE KEYSPACE IF NOT EXISTS <TOKEN>_analysis WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1' : 2, 'DC2': 3, 'DC3': 2};\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dse.cluster import Cluster\n",
    "    from dse.auth import DSEPlainTextAuthProvider\n",
    "except ImportError:\n",
    "    from cassandra.cluster import Cluster\n",
    "    from cassandra.auth import DSEPlainTextAuthProvider\n",
    "\n",
    "# auth_provider = DSEPlainTextAuthProvider('cassandra', 'xxxx')\n",
    "\n",
    "#cluster = Cluster(['10.154.0.6', '10.154.0.3'], auth_provider=auth_provider)  # provide contact points and port\n",
    "cluster = Cluster(['127.0.0.1'])  # provide contact points and port\n",
    "session = cluster.connect('xalperte_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dse.cqlengine.models import Model\n",
    "    from dse.cqlengine import columns\n",
    "    from dse.cqlengine.management import sync_table\n",
    "    from dse.cqlengine.connection import register_connection\n",
    "except ImportError:\n",
    "    from cassandra.cqlengine.models import Model\n",
    "    from cassandra.cqlengine import columns\n",
    "    from cassandra.cqlengine.management import sync_table\n",
    "    from cassandra.cqlengine.connection import register_connection\n",
    "    \n",
    "from pyspark.sql.types import StringType, DateType, TimestampType\n",
    "from pyspark.sql.types import FloatType, DecimalType, DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def create_cassandra_model(schema, table_name, primary_colums=None):\n",
    "    class MyModel(object):\n",
    "        pass\n",
    "\n",
    "    if not isinstance(primary_colums, list):\n",
    "        primary_colums = [primary_colums]\n",
    "        \n",
    "    df_fields = {}\n",
    "    for field in schema.fields:\n",
    "        df_fields[field.name] = field\n",
    "        \n",
    "    metadata = {\n",
    "        \"__table_name__\": table_name\n",
    "    }\n",
    "        \n",
    "    # First the primary keys\n",
    "    for field_name in primary_colums:\n",
    "        field = df_fields[field_name]\n",
    "        if isinstance(field.dataType, StringType):\n",
    "            metadata[field_name] = columns.Text(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, DateType):\n",
    "            metadata[field_name] = columns.Date(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, TimestampType):\n",
    "            metadata[field_name] = columns.DateTime(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, DoubleType):\n",
    "            metadata[field_name] = columns.Double(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, DecimalType):\n",
    "            metadata[field_name] = columns.Decimal(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, FloatType):\n",
    "            metadata[field_name] = columns.Float(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, IntegerType):\n",
    "            metadata[field_name] = columns.Integer(required=(not field.nullable), primary_key=True)\n",
    "\n",
    "    # First the primary keys\n",
    "    for field_name, field in df_fields.items():\n",
    "        if field_name not in primary_colums:\n",
    "            if isinstance(field.dataType, StringType):\n",
    "                metadata[field_name] = columns.Text(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, DateType):\n",
    "                metadata[field_name] = columns.Date(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, TimestampType):\n",
    "                metadata[field_name] = columns.DateTime(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, DoubleType):\n",
    "                metadata[field_name] = columns.Double(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, DecimalType):\n",
    "                metadata[field_name] = columns.Decimal(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, FloatType):\n",
    "                metadata[field_name] = columns.Float(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, IntegerType):\n",
    "                metadata[field_name] = columns.Integer(required=(not field.nullable), primary_key=False)\n",
    "\n",
    "    return type('MyModel', (Model,), metadata)\n",
    "            \n",
    "            \n",
    "model = create_cassandra_model(factors_df_standard.schema, \"capex_vol_6q\", \"asset\")\n",
    "register_connection(\"my_connection\", session=session)\n",
    "sync_table(model, keyspaces=[\"xalperte_analysis\"], connections=[\"my_connection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"capex_vol_6q\", keyspace=\"xalperte_analysis\")\\\n",
    "    .option(\"confirm.truncate\",\"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .partitionBy(\"astodate\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_data.set_index(['astodate', 'asset'], inplace=True)\n",
    "display(factors_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unixt_factors_data = factors_data.set_index(pd.MultiIndex.from_tuples(\n",
    "    [(x.timestamp(), y) for x, y in factors_data.index.values],\n",
    "    names=['astodate', 'asset']))\n",
    "display(unixt_factors_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
