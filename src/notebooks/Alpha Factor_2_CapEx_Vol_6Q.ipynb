{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Spark + DSE\n",
    "\n",
    "In BGDS, we advocate for the convination of the following tools as the best solution to address Big data's problems.\n",
    "\n",
    "- Apache Spark\n",
    "- Apache Cassandra (DSE)\n",
    "- Apache Solr (DSE)\n",
    "\n",
    "This notebook is an example of how to use these tools together to analyze the data crawled by the `Bovespa` example crawler included in the DaVinci distribution ()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 1.1 ms\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.auto_scroll_threshold = 9999\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 381 µs\n"
     ]
    }
   ],
   "source": [
    "# Configurations related to Cassandra connector & Cluster\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "    '--packages datastax:spark-cassandra-connector:2.4.0-s_2.11 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Factor Analysis Job, master=local) created by __init__ at <ipython-input-3-1410226d81b3>:13 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2e98fb7c0e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mspark_master\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"local\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_master\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Factor Analysis Job\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Factor Analysis Job, master=local) created by __init__ at <ipython-input-3-1410226d81b3>:13 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 51.5 ms\n"
     ]
    }
   ],
   "source": [
    "# Creating Spark Context\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf() \\\n",
    " .setAppName(\"Factor Analysis Job\") \\\n",
    " .set(\"spark.cassandra.connection.host\", \"tfm_uoc_dse\") \\\n",
    " .set(\"spark.sql.dse.search.enableOptimization\", \"on\") \n",
    "# .set(\"spark.cassandra.auth.username\", \"cassandra\") \\\n",
    "# .set(\"spark.cassandra.auth.password\", \"*********\")  \n",
    "\n",
    "spark_master = \"local\"\n",
    "sc = SparkContext(spark_master, \"Factor Analysis Job\", conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.43 ms\n"
     ]
    }
   ],
   "source": [
    "# Creating PySpark SQL Context\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 504 µs\n"
     ]
    }
   ],
   "source": [
    "# Utility function for return a data frame attached to the informed keyspace.table\n",
    "def load_and_get_table_df(keys_space_name, table_name):\n",
    "    table_df = sqlContext.read \\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(table=table_name, keyspace=keys_space_name) \\\n",
    "        .option(\"spark.sql.dse.search.enableOptimization\", \"on\") \\\n",
    "        .load()\n",
    "    return table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------+------------------+--------------------+------------+--------------------+--------------+------------+----------+---------+----------+--------------------+\n",
      "|entity_type| ccvm|canceled_date|              cnpj|        company_name|company_type|          created_at|deleted_reason|granted_date|is_deleted|situation|solr_query|          updated_at|\n",
      "+-----------+-----+-------------+------------------+--------------------+------------+--------------------+--------------+------------+----------+---------+----------+--------------------+\n",
      "|    company| 1023|         null|00.000.000/0001-91|BANCO DO BRASIL S.A.|CIAS ABERTAS|2019-12-26 02:11:...|          null|  1977-07-20|     false|  GRANTED|      null|2019-12-26 02:11:...|\n",
      "|    company|10243|         null|83.296.889/0001-23|MASSA FALIDA DA S...|CIAS ABERTAS|2019-12-26 02:12:...|          null|  1979-11-30|     false|  GRANTED|      null|2019-12-26 02:12:...|\n",
      "|    company|10456|         null|61.079.117/0001-05|       ALPARGATAS SA|CIAS ABERTAS|2019-12-26 00:41:...|          null|  1977-07-20|     false|  GRANTED|      null|2019-12-26 00:41:...|\n",
      "|    company|10472|         null|60.500.139/0001-26|SARAIVA SA LIVREI...|CIAS ABERTAS|2019-12-26 02:13:...|          null|  1977-07-20|     false|  GRANTED|      null|2019-12-26 02:13:...|\n",
      "|    company|10561|         null|87.043.832/0001-73|SEIVA S.A. - FLOR...|CIAS ABERTAS|2019-12-26 02:13:...|          null|  1981-11-12|     false|  GRANTED|      null|2019-12-26 02:13:...|\n",
      "|    company|10880|         null|33.386.210/0001-19|SONDOTECNICA ENGE...|CIAS ABERTAS|2019-12-26 02:13:...|          null|  1980-08-19|     false|  GRANTED|      null|2019-12-26 02:13:...|\n",
      "|    company|10960|         null|92.929.520/0001-00|         SPRINGER SA|CIAS ABERTAS|2019-12-26 02:13:...|          null|  1968-10-24|     false|  GRANTED|      null|2019-12-26 02:13:...|\n",
      "|    company|11070|         null|33.228.024/0001-51|WLM INDÚSTRIA E C...|CIAS ABERTAS|2019-12-26 02:13:...|          null|  1971-07-01|     false|  GRANTED|      null|2019-12-26 02:13:...|\n",
      "|    company| 1120|         null|13.009.717/0001-46|BANCO DO ESTADO D...|CIAS ABERTAS|2019-12-26 02:11:...|          null|  1977-07-20|     false|  GRANTED|      null|2019-12-26 02:11:...|\n",
      "|    company|11207|         null|33.111.246/0001-90|TECNOSOLO S/A - E...|CIAS ABERTAS|2019-12-26 02:13:...|          null|  1977-07-20|     false|  GRANTED|      null|2019-12-26 02:13:...|\n",
      "+-----------+-----+-------------+------------------+--------------------+------------+--------------------+--------------+------------+----------+---------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "time: 190 ms\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# Loading Bovespa companies table\n",
    "companies = load_and_get_table_df(\"tfm_uoc\", \"bovespa_company\")\n",
    "companies.filter(~col(\"situation\").eqNullSafe(\"CANCELED\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter NOT ('situation <=> CANCELED)\n",
      "+- Relation[entity_type#1011,ccvm#1012,canceled_date#1013,cnpj#1014,company_name#1015,company_type#1016,created_at#1017,deleted_reason#1018,granted_date#1019,is_deleted#1020,situation#1021,solr_query#1022,updated_at#1023] org.apache.spark.sql.cassandra.CassandraSourceRelation@3e768c0f\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "entity_type: string, ccvm: string, canceled_date: date, cnpj: string, company_name: string, company_type: string, created_at: timestamp, deleted_reason: string, granted_date: date, is_deleted: boolean, situation: string, solr_query: string, updated_at: timestamp\n",
      "Filter NOT (situation#1021 <=> CANCELED)\n",
      "+- Relation[entity_type#1011,ccvm#1012,canceled_date#1013,cnpj#1014,company_name#1015,company_type#1016,created_at#1017,deleted_reason#1018,granted_date#1019,is_deleted#1020,situation#1021,solr_query#1022,updated_at#1023] org.apache.spark.sql.cassandra.CassandraSourceRelation@3e768c0f\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Filter NOT (situation#1021 <=> CANCELED)\n",
      "+- Relation[entity_type#1011,ccvm#1012,canceled_date#1013,cnpj#1014,company_name#1015,company_type#1016,created_at#1017,deleted_reason#1018,granted_date#1019,is_deleted#1020,situation#1021,solr_query#1022,updated_at#1023] org.apache.spark.sql.cassandra.CassandraSourceRelation@3e768c0f\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Filter NOT (situation#1021 <=> CANCELED)\n",
      "+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@3e768c0f [entity_type#1011,ccvm#1012,canceled_date#1013,cnpj#1014,company_name#1015,company_type#1016,created_at#1017,deleted_reason#1018,granted_date#1019,is_deleted#1020,situation#1021,solr_query#1022,updated_at#1023] PushedFilters: [Not(EqualNullSafe(situation,CANCELED))], ReadSchema: struct<entity_type:string,ccvm:string,canceled_date:date,cnpj:string,company_name:string,company_...\n",
      "time: 115 ms\n"
     ]
    }
   ],
   "source": [
    "# Check the execution plan of the query\n",
    "companies.filter(~col(\"situation\").eqNullSafe(\"CANCELED\")).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'cap_ex_reported'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5add30323a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m factors_df = factors_df.withColumn(\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m\"cap_ex_reported_scaled\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     when(factors_df.cap_ex_reported > 0, 0).otherwise(abs(factors_df.cap_ex_reported)) / factors_df.total_assets)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m factors_df = factors_df.withColumn(\"capex_vol_6q\", stddev(col(\"cap_ex_reported_scaled\"))\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1301\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1302\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'cap_ex_reported'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Accessing to the fundamental data of the companies\n",
    "# Attaching a data frame to the accounting notes available in the database\n",
    "fundamentals = load_and_get_table_df(\"tfm_uoc\", \"bovespa_account\")\n",
    "\n",
    "# Get the Capital Expenditures from the Fundamentals\n",
    "# Account 1    = Total Assets of the company\n",
    "# Account 6.02 = Capital Expenditure reported by the company during the period\n",
    "factors_df = fundamentals.filter(\n",
    "    col(\"number\").isin([\"6.02\", \"1\"]))\n",
    "factors_df = factors_df.withColumn(\n",
    "    \"factor_name\", when(factors_df.number == \"1\", \"total_assets\").otherwise(\"cap_ex_reported\"))\n",
    "\n",
    "factors_df = factors_df\\\n",
    "    .select(col(\"ccvm\").alias(\"asset\"), \n",
    "            col(\"period\").alias(\"astodate\"),\n",
    "            col(\"factor_name\"), \n",
    "            col(\"amount\").alias(\"amount\"))\n",
    "\n",
    "factors_df = factors_df.groupby(col(\"asset\"), col(\"astodate\"))\\\n",
    "    .pivot(\"factor_name\").sum(\"amount\").orderBy(\"asset\", \"astodate\")\n",
    "\n",
    "factors_df = factors_df.withColumn(\n",
    "    \"cap_ex_reported_scaled\", \n",
    "    when(factors_df.cap_ex_reported > 0, 0).otherwise(abs(factors_df.cap_ex_reported)) / factors_df.total_assets)\n",
    "\n",
    "factors_df = factors_df.withColumn(\"capex_vol_6q\", stddev(col(\"cap_ex_reported_scaled\"))\n",
    "             .over(Window.partitionBy(\"asset\").rowsBetween(-5, 0)) )\n",
    "\n",
    "factors_df = factors_df.withColumn(\"capex_vol_6q_ranked\", rank()\\\n",
    "             .over(Window.partitionBy(\"astodate\").orderBy(asc(\"capex_vol_6q\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df.select(col(\"astodate\"), col(\"asset\"), col(\"capex_vol_6q\"), col(\"capex_vol_6q_ranked\")).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_w(col, w):\n",
    "    avg_ = avg(col).over(w)\n",
    "    avg_sq = avg(col * col).over(w)\n",
    "    sd_ = sqrt(avg_sq - avg_ * avg_)\n",
    "    return (col - avg_) / sd_\n",
    "\n",
    "w = Window().partitionBy(\"astodate\")\n",
    "factors_df_standard = factors_df.withColumn(\"capex_vol_6q_ranked_zscored\", \n",
    "                                            z_score_w(factors_df.capex_vol_6q_ranked, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df_standard.select(\n",
    "    col(\"astodate\"), \n",
    "    col(\"asset\"), \n",
    "    col(\"capex_vol_6q_ranked_zscored\")).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_data = factors_df_standard.select(\n",
    "    col(\"astodate\"), \n",
    "    col(\"asset\"), \n",
    "    col(\"capex_vol_6q_ranked_zscored\").alias(\"capex_vol_6q\")).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist the results\n",
    "\n",
    "Now it's time to save the factor into a table. To do this, the system should had create a \n",
    "keyspace for us in the database. The keyspace should have the same TOKEN than the one used\n",
    "to connect through the API.\n",
    "\n",
    "Each user will have it's own space in the system to run the analyses.\n",
    "\n",
    "```sql\n",
    "CREATE KEYSPACE IF NOT EXISTS <USER_ID>_analysis WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1};\n",
    "```\n",
    "\n",
    "with full access to it:\n",
    "\n",
    "```sql\n",
    "-- Create the user using the token and\n",
    "CREATE ROLE user_<USER_ID> WITH PASSWORD = '<USER_TOKEN>' AND LOGIN = true AND SUPERUSER = false;\n",
    "\n",
    "-- Grant all permission for the user keyspace\n",
    "GRANT ALL PERMISSIONS IN KEYSPACE <USER_ID>_analysis to user_<USER_ID>;\n",
    "```\n",
    "\n",
    "PRO features:\n",
    "- Allow replication of data (ReplicationFactor = 3)\n",
    "    ```sql\n",
    "    CREATE KEYSPACE IF NOT EXISTS <TOKEN>_analysis WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 3};\n",
    "    ```\n",
    "- Network replication (multiple DataCenters)\n",
    "    ```sql\n",
    "    CREATE KEYSPACE IF NOT EXISTS <TOKEN>_analysis WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1' : 2, 'DC2': 3, 'DC3': 2};\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dse.cluster import Cluster\n",
    "    from dse.auth import DSEPlainTextAuthProvider\n",
    "except ImportError:\n",
    "    from cassandra.cluster import Cluster\n",
    "    from cassandra.auth import DSEPlainTextAuthProvider\n",
    "\n",
    "cluster = Cluster(['tfm_uoc_dse'])  # provide contact points and port\n",
    "session = cluster.connect('xalperte_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dse.cqlengine.models import Model\n",
    "    from dse.cqlengine import columns\n",
    "    from dse.cqlengine.management import sync_table\n",
    "    from dse.cqlengine.connection import register_connection\n",
    "except ImportError:\n",
    "    from cassandra.cqlengine.models import Model\n",
    "    from cassandra.cqlengine import columns\n",
    "    from cassandra.cqlengine.management import sync_table\n",
    "    from cassandra.cqlengine.connection import register_connection\n",
    "    \n",
    "from pyspark.sql.types import StringType, DateType, TimestampType\n",
    "from pyspark.sql.types import FloatType, DecimalType, DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def create_cassandra_model(schema, table_name, primary_colums=None):\n",
    "    class MyModel(object):\n",
    "        pass\n",
    "\n",
    "    if not isinstance(primary_colums, list):\n",
    "        primary_colums = [primary_colums]\n",
    "        \n",
    "    df_fields = {}\n",
    "    for field in schema.fields:\n",
    "        df_fields[field.name] = field\n",
    "        \n",
    "    metadata = {\n",
    "        \"__table_name__\": table_name\n",
    "    }\n",
    "        \n",
    "    # First the primary keys\n",
    "    for field_name in primary_colums:\n",
    "        field = df_fields[field_name]\n",
    "        if isinstance(field.dataType, StringType):\n",
    "            metadata[field_name] = columns.Text(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, DateType):\n",
    "            metadata[field_name] = columns.Date(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, TimestampType):\n",
    "            metadata[field_name] = columns.DateTime(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, DoubleType):\n",
    "            metadata[field_name] = columns.Double(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, DecimalType):\n",
    "            metadata[field_name] = columns.Decimal(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, FloatType):\n",
    "            metadata[field_name] = columns.Float(required=(not field.nullable), primary_key=True)\n",
    "        if isinstance(field.dataType, IntegerType):\n",
    "            metadata[field_name] = columns.Integer(required=(not field.nullable), primary_key=True)\n",
    "\n",
    "    # First the primary keys\n",
    "    for field_name, field in df_fields.items():\n",
    "        if field_name not in primary_colums:\n",
    "            if isinstance(field.dataType, StringType):\n",
    "                metadata[field_name] = columns.Text(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, DateType):\n",
    "                metadata[field_name] = columns.Date(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, TimestampType):\n",
    "                metadata[field_name] = columns.DateTime(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, DoubleType):\n",
    "                metadata[field_name] = columns.Double(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, DecimalType):\n",
    "                metadata[field_name] = columns.Decimal(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, FloatType):\n",
    "                metadata[field_name] = columns.Float(required=(not field.nullable), primary_key=False)\n",
    "            if isinstance(field.dataType, IntegerType):\n",
    "                metadata[field_name] = columns.Integer(required=(not field.nullable), primary_key=False)\n",
    "\n",
    "    return type('MyModel', (Model,), metadata)\n",
    "            \n",
    "            \n",
    "model = create_cassandra_model(factors_df_standard.schema, \"capex_vol_6q\", \"asset\")\n",
    "register_connection(\"my_connection\", session=session)\n",
    "sync_table(model, keyspaces=[\"xalperte_analysis\"], connections=[\"my_connection\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_df.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"capex_vol_6q\", keyspace=\"xalperte_analysis\")\\\n",
    "    .option(\"confirm.truncate\",\"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .partitionBy(\"astodate\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_data.set_index(['astodate', 'asset'], inplace=True)\n",
    "display(factors_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unixt_factors_data = factors_data.set_index(pd.MultiIndex.from_tuples(\n",
    "    [(x.timestamp(), y) for x, y in factors_data.index.values],\n",
    "    names=['astodate', 'asset']))\n",
    "display(unixt_factors_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
