{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.auto_scroll_threshold = 9999\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "import pandas_market_calendars as mcal\n",
    "\n",
    "from financials import group_by_period\n",
    "\n",
    "\n",
    "# Bovespa calendar\n",
    "bmf_calendar = mcal.get_calendar('BMF')\n",
    "\n",
    "early = bmf_calendar.schedule(start_date='2000-01-01', end_date=datetime.utcnow())\n",
    "\n",
    "daily_dates = mcal.date_range(early, frequency='1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2000-01-03 18:00:00+00:00\n",
       "1      2000-01-04 18:00:00+00:00\n",
       "2      2000-01-05 18:00:00+00:00\n",
       "3      2000-01-06 18:00:00+00:00\n",
       "4      2000-01-07 18:00:00+00:00\n",
       "                  ...           \n",
       "4954   2020-01-03 19:00:00+00:00\n",
       "4955   2020-01-06 19:00:00+00:00\n",
       "4956   2020-01-07 19:00:00+00:00\n",
       "4957   2020-01-08 19:00:00+00:00\n",
       "4958   2020-01-09 19:00:00+00:00\n",
       "Name: index, Length: 4959, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15.8 ms\n"
     ]
    }
   ],
   "source": [
    "daily_dates.to_frame().reset_index()[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Trading Calendars Job, master=local) created by __init__ at /home/jovyan/work/notebooks/spark.py:22 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c5c136e73dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_spark_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_and_get_table_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trading Calendars Job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/notebooks/spark.py\u001b[0m in \u001b[0;36minit_spark_context\u001b[0;34m(context_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mspark_master\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"local\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_master\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msql_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Trading Calendars Job, master=local) created by __init__ at /home/jovyan/work/notebooks/spark.py:22 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 72.6 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from db import sync_table\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from spark import init_spark_context, load_and_get_table_df\n",
    "\n",
    "sc, sql_context = init_spark_context(\"Trading Calendars Job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 83.6 ms\n"
     ]
    }
   ],
   "source": [
    "trading_dates_schema = StructType([\n",
    "    StructField(\"date\", DateType(), True)])   \n",
    "\n",
    "trading_dates_spark_df = sql_context.createDataFrame(\n",
    "    daily_dates.to_frame(),\n",
    "    schema=trading_dates_schema)\n",
    "\n",
    "trading_dates_spark_df = trading_dates_spark_df.withColumn(\"calendar\", F.lit(\"BMF\"))\n",
    "\n",
    "sync_table(trading_dates_spark_df, \"tfm_uoc_dse\", \"tfm_uoc_analysis\", \"daily_trading_dates\", [\"calendar\", \"date\"])\n",
    "\n",
    "trading_dates_spark_df.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"daily_trading_dates\", keyspace=\"tfm_uoc_analysis\")\\\n",
    "    .option(\"confirm.truncate\",\"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .partitionBy(\"astodate\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/dse/cqlengine/management.py:540: UserWarning: CQLENG_ALLOW_SCHEMA_MANAGEMENT environment variable is not set. Future versions of this package will require this variable to enable management functions.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing connections\n",
      "time: 7.39 s\n"
     ]
    }
   ],
   "source": [
    "monthly_trading_dates_df = group_by_period(\n",
    "    trading_dates_spark_df,\n",
    "    columns=[\"date\", \"date\"],\n",
    "    columns_aliases=[\"first_date\", \"last_date\"],\n",
    "    columns_agg=[F.first, F.last],\n",
    "    date_field=\"date\",\n",
    "    group_columns=[\"calendar\"],\n",
    "    frequency=\"monthly\"\n",
    ")\n",
    "\n",
    "sync_table(monthly_trading_dates_df, \n",
    "           \"tfm_uoc_dse\", \"tfm_uoc_analysis\", \"monthly_trading_dates\", \n",
    "           [\"calendar\", \"year\", \"month\"])\n",
    "\n",
    "monthly_trading_dates_df.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"monthly_trading_dates\", keyspace=\"tfm_uoc_analysis\")\\\n",
    "    .option(\"confirm.truncate\",\"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .partitionBy(\"astodate\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing connections\n",
      "time: 5.61 s\n"
     ]
    }
   ],
   "source": [
    "quarterly_trading_dates_df = group_by_period(\n",
    "    trading_dates_spark_df,\n",
    "    columns=[\"date\", \"date\"],\n",
    "    columns_aliases=[\"first_date\", \"last_date\"],\n",
    "    columns_agg=[F.first, F.last],\n",
    "    date_field=\"date\",\n",
    "    group_columns=[\"calendar\"],\n",
    "    frequency=\"quarterly\"\n",
    ")\n",
    "\n",
    "sync_table(quarterly_trading_dates_df, \n",
    "           \"tfm_uoc_dse\", \"tfm_uoc_analysis\", \"quarterly_trading_dates\", \n",
    "           [\"calendar\", \"year\", \"quarter\"])\n",
    "\n",
    "quarterly_trading_dates_df.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"quarterly_trading_dates\", keyspace=\"tfm_uoc_analysis\")\\\n",
    "    .option(\"confirm.truncate\",\"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .partitionBy(\"astodate\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing connections\n",
      "time: 7.7 s\n"
     ]
    }
   ],
   "source": [
    "yearly_trading_dates_df = group_by_period(\n",
    "    trading_dates_spark_df,\n",
    "    columns=[\"date\", \"date\"],\n",
    "    columns_aliases=[\"first_date\", \"last_date\"],\n",
    "    columns_agg=[F.first, F.last],\n",
    "    date_field=\"date\",\n",
    "    group_columns=[\"calendar\"],\n",
    "    frequency=\"yearly\"\n",
    ")\n",
    "\n",
    "sync_table(yearly_trading_dates_df, \n",
    "           \"tfm_uoc_dse\", \"tfm_uoc_analysis\", \"yearly_trading_dates\", \n",
    "           [\"calendar\", \"year\"])\n",
    "\n",
    "yearly_trading_dates_df.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"yearly_trading_dates\", keyspace=\"tfm_uoc_analysis\")\\\n",
    "    .option(\"confirm.truncate\",\"true\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .partitionBy(\"astodate\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 827 ms\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
